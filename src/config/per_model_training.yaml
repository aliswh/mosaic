llama-3b: 
  learning_rate: 0.0001 
  weight_decay: 0.001

llama-8b: 
  learning_rate: 0.0001 
  weight_decay: 0.001

mmed-llama-8b:
  learning_rate: 0.0001 
  weight_decay: 0.001

gemma-4b:
  learning_rate: 0.0001  
  weight_decay: 0.01

medgemma-4b:
  learning_rate: 0.0001  
  weight_decay: 0.01

gemma-12b:
  learning_rate: 0.0005  
  weight_decay: 0.01