# if lora rank is 0, full fine-tuning

llama-3b: 
  model_tag: "unsloth/Llama-3.2-3B-Instruct" 
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64 
  lora_alpha: 128
  model_family: llama
  chat_template: "llama-3.1"
  batch_size: 8

llama-8b: 
  model_tag: "unsloth/Llama-3.1-8B-Instruct" 
  load_in_4bit: true 
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: llama
  chat_template: "llama-3.1"
  batch_size: 8
  load_adapter: false

llama-70b: 
  model_tag: "unsloth/Llama-3.3-70B-Instruct"
  load_in_4bit: true 
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: llama
  chat_template: "llama-3.1"
  batch_size: 32
  load_adapter: false

mmed-llama-8b:
  model_tag: "Henrychur/MMed-Llama-3-8B"
  load_in_4bit: false # doesn't have a 4bit version
  max_seq_length: 2048
  load_in_8bit: false
  lora_rank: 64
  lora_alpha: 128
  model_family: llama
  load_adapter: false
  chat_template: "llama-3.1"
  batch_size: 8

gemma-4b:
  model_tag: "unsloth/gemma-3-4b-it"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128 # 64
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 16

medgemma-4b:
  model_tag: "unsloth/medgemma-4b-it"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 16

gemma-12b:
  model_tag: "unsloth/gemma-3-12b-it"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 4

gemma-27b:
  model_tag: "unsloth/gemma-3-27b-it"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 32

medgemma-27b:
  model_tag: "unsloth/medgemma-27b-text-it"
  max_seq_length: 2048
  load_in_4bit: true
  load_in_8bit: false
  lora_rank: 64
  lora_alpha: 64
  model_family: gemma
  load_adapter: false
  chat_template: "gemma-3"
  batch_size: 32

# MOSAIC models

mosaic-4b:
  model_tag: "AliceSch/mosaic-4b"
  load_in_4bit: false
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 16

mosaic-12b:
  model_tag: "AliceSch/mosaic-12b"
  load_in_4bit: false
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 4