# if lora rank is 0, full fine-tuning

llama-3b: 
  model_tag: "unsloth/Llama-3.2-3B" # "meta-llama/Llama-3.2-3B-Instruct"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64 
  lora_alpha: 128
  model_family: llama
  chat_template: "llama-3.1"
  batch_size: 8

llama-8b: 
  model_tag: "unsloth/Llama-3.1-8B-Instruct" 
  load_in_4bit: true 
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: llama
  chat_template: "llama-3.1"
  batch_size: 8
  load_adapter: false

llama-70b: 
  model_tag: "unsloth/Llama-3.3-70B-Instruct"
  load_in_4bit: true 
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: llama
  chat_template: "llama-3.1"
  batch_size: 32
  load_adapter: false

mmed-llama-8b:
  model_tag: "Henrychur/MMed-Llama-3-8B"
  load_in_4bit: false # doesn't have a 4bit version
  max_seq_length: 2048
  load_in_8bit: false
  lora_rank: 64
  lora_alpha: 128
  model_family: llama
  load_adapter: false
  chat_template: "llama-3.1"
  batch_size: 8

gemma-4b:
  model_tag: "unsloth/gemma-3-4b-it"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128 # 64
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 16
  config_path: "/home/alice/.cache/huggingface/hub/models--unsloth--gemma-3-4b-it/snapshots/b2e2d5668e652e0d0fe9cb9ae42a061e33cb256d/config.json"

medgemma-4b:
  model_tag: "unsloth/medgemma-4b-it"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 16
  config_path: "/home/alice/.cache/huggingface/hub/models--google--medgemma-4b-it/snapshots/698f7911b8e0569ff4ebac5d5552f02a9553063c/config.json"

gemma-12b:
  model_tag: "unsloth/gemma-3-12b-it"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 4
  config_path: "/home/alice/.cache/huggingface/hub/models--unsloth--gemma-3-12b-it/snapshots/1b50bd0b78e6af2c4ea9e9bf220ac2b9a7a22655/config.json"

gemma-27b:
  model_tag: "unsloth/gemma-3-27b-it"
  load_in_4bit: true
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 32
  config_path: "/home/alice/.cache/huggingface/hub/models--unsloth--gemma-3-27b-it/snapshots/1b50bd0b78e6af2c4ea9e9bf220ac2b9a7a22655/config.json"

medgemma-27b:
  model_tag: "unsloth/medgemma-27b-text-it"
  max_seq_length: 2048
  load_in_4bit: true
  load_in_8bit: false
  lora_rank: 64
  lora_alpha: 64
  model_family: gemma
  load_adapter: false
  config_path: "/home/alice/.cache/huggingface/hub/models--unsloth--gemma-3-12b-it/snapshots/1b50bd0b78e6af2c4ea9e9bf220ac2b9a7a22655/config.json"
  chat_template: "gemma-3"
  batch_size: 32

# 

mosaic-4b:
  model_tag: "AliceSch/mosaic-4b"
  load_in_4bit: false
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 16

mosaic-12b:
  model_tag: "AliceSch/mosaic-12b"
  load_in_4bit: false
  max_seq_length: 2048
  lora_rank: 64
  lora_alpha: 128
  model_family: gemma
  chat_template: "gemma-3"
  batch_size: 4
  config_path: "/home/alice/.cache/huggingface/hub/models--unsloth--gemma-3-12b-it/snapshots/1b50bd0b78e6af2c4ea9e9bf220ac2b9a7a22655/config.json"
